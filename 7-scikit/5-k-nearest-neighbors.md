# ğŸ§  Topic: K-Nearest Neighbors (KNN)

---

## âš™ï¸ 1ï¸âƒ£ What is K-Nearest Neighbors?

KNN is a **supervised learning algorithm** that can perform both **classification** and **regression**.

Instead of building a mathematical model during training, it **stores all the training data** and makes predictions based on the **similarity (distance)** between new and existing points.

ğŸ’¡ **Think of it like this:**
You move into a new neighborhood and want to guess your likely income level. You look at your *K nearest neighbors* â€” if most of them earn high, your predicted income is likely high too.

---

## ğŸ” 2ï¸âƒ£ How KNN Works (Step-by-Step)

Letâ€™s say you want to predict whether a new student will **pass or fail** based on **study hours** and **sleep hours**.

1. **Choose a value for K** (e.g., 3)
2. **Calculate the distance** between the new student and all others in the dataset
   (usually Euclidean distance)
3. **Find the K nearest neighbors**
4. **Vote (classification)** â†’ take the most common label among neighbors
   or
   **Average (regression)** â†’ take the mean of neighbor values

---

### âœ³ï¸ Example Diagram (Conceptually)

Imagine points plotted on a 2D graph:

* ğŸ”µ = Pass
* ğŸ”´ = Fail

When you add a new point âŒ, KNN looks at the 3 nearest points.
If 2 out of 3 are ğŸ”µ, the new point is predicted as â€œPass.â€

---

## ğŸ“ 3ï¸âƒ£ Distance Metrics

The **distance metric** defines â€œclosenessâ€ between points.

| Distance Type         | Formula                                  | When to Use                   |   |           |   |                              |
| --------------------- | ---------------------------------------- | ----------------------------- | - | --------- | - | ---------------------------- |
| **Euclidean**         | ( \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} ) | Default for numeric data      |   |           |   |                              |
| **Manhattan**         | (                                        | x_1 - x_2                     | + | y_1 - y_2 | ) | Grid-like or city-block data |
| **Minkowski**         | Generalization of both                   | Mixed data types              |   |           |   |                              |
| **Cosine Similarity** | Measures angle, not distance             | Text or high-dimensional data |   |           |   |                              |

---

## ğŸ§® 4ï¸âƒ£ Example: Classification with KNN

```python
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd

# Dataset
data = pd.DataFrame({
    'Hours_Studied': [1, 2, 3, 4, 5, 6],
    'Sleep_Hours': [9, 8, 7, 6, 5, 4],
    'Pass': [0, 0, 0, 1, 1, 1]
})

X = data[['Hours_Studied', 'Sleep_Hours']]
y = data['Pass']

# Model
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X, y)

# Predict for a new student
pred = model.predict([[3.5, 6]])
print(pred)  # Output: [1] â†’ Predicted Pass
```

---

## ğŸ“Š 5ï¸âƒ£ Choosing the Right K

* If **K is too small** â†’ Model becomes sensitive to noise (overfits)
* If **K is too large** â†’ Model becomes too smooth (underfits)

âœ… A common trick:
Use **odd K values** (like 3, 5, 7) to avoid ties in classification.

To choose the best K, use **cross-validation**.

---

## âš¡ 6ï¸âƒ£ KNN for Regression

Instead of voting, KNN Regression averages the numeric values of the K nearest neighbors.

```python
from sklearn.neighbors import KNeighborsRegressor

model = KNeighborsRegressor(n_neighbors=3)
model.fit(X, y)
pred = model.predict([[3.5, 6]])
print(pred)
```

---

## âš™ï¸ 7ï¸âƒ£ Data Scaling is Crucial âš ï¸

Since KNN relies on distance, **features with large ranges dominate** the result.
So, always scale your data using StandardScaler or MinMaxScaler before applying KNN.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

---

## ğŸ§  Socratic Questions

1. Why does KNN not have a traditional â€œtrainingâ€ phase?
2. What might happen if one feature (like Salary) is much larger in scale than another (like Age)?
3. How would increasing K affect model accuracy and bias?
4. Why do we often use odd values for K?

---

## ğŸ’ª Mini Exercise

Create a small dataset:

| Study Hours | Sleep Hours | Pass |
| ----------- | ----------- | ---- |
| 1           | 9           | 0    |
| 2           | 8           | 0    |
| 3           | 7           | 0    |
| 4           | 6           | 1    |
| 5           | 5           | 1    |
| 6           | 4           | 1    |

Tasks:

1. Train a KNN classifier with `n_neighbors = 3`.
2. Predict for a new student who studied `3.5` hours and slept `6` hours.
3. Try changing K = 1, 3, 5 â€” observe how predictions change.
4. Apply `StandardScaler` and see if the result differs.

---

## ğŸ§© Bonus Insight

âœ… **Pros**

* Simple, intuitive, no training time
* Works for both regression & classification
* Handles non-linear relationships

âŒ **Cons**

* Slow for large datasets (since it checks all neighbors)
* Sensitive to irrelevant or unscaled features
* Doesnâ€™t handle high dimensions well (curse of dimensionality)
